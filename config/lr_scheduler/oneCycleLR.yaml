# @package _global_
lr_scheduler:
  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    max_lr: 1e-3
    total_steps: 19050 #127*32 # Total training steps
    pct_start: 0.3
    anneal_strategy: cos
    div_factor: 100.0
    final_div_factor: 1e4
  interval: step
#initial = 1e-5: max/initial = div_factor

#  MORE AGGRESSIVE
# lr_scheduler:
#   scheduler:
#     _target_: torch.optim.lr_scheduler.OneCycleLR
#     max_lr: 1e-3
#     total_steps: 3000000 # Total training steps
#     pct_start: 0.1 # Faster warmup
#     anneal_strategy: cos
#     div_factor: 10.0 # More aggressive initial learning rate
#     final_div_factor: 1e3 # Higher ending learning rate
#   interval: step
